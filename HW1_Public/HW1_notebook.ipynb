{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF3RO09kuPxc"
   },
   "source": [
    "# Problem A-i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKHDa9HFuPxe"
   },
   "source": [
    "Use this notebook to write your code for perceptron by filling in the sections marked `# TODO` and running all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "ZQfV0puDuPxe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from perceptron_helper import (\n",
    "    predict,\n",
    "    plot_data,\n",
    "    boundary,\n",
    "    plot_perceptron,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyKMZv2iuPxe"
   },
   "source": [
    "## Implementation of Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEHyif1KuPxe"
   },
   "source": [
    "First, we will implement the perceptron algorithm. Fill in the `update_perceptron()` function so that it finds a single misclassified point and updates the weights and bias accordingly. If no point exists, the weights and bias should not change.\n",
    "\n",
    "Hint: You can use the `predict()` helper method, which labels a point 1 or -1 depending on the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "h12yLv2OuPxe"
   },
   "outputs": [],
   "source": [
    "def update_perceptron(X, Y, w, b):\n",
    "    \"\"\"\n",
    "    This method updates a perceptron model. Takes in the previous weights\n",
    "    and returns weights after an update, which could be nothing.\n",
    "\n",
    "    Inputs:\n",
    "        X: A (N, D) shaped numpy array containing a single point.\n",
    "        Y: A (N, ) shaped numpy array containing the labels for the points.\n",
    "        w: A (D, ) shaped numpy array containing the weight vector.\n",
    "        b: A float containing the bias term.\n",
    "\n",
    "    Output:\n",
    "        next_w: A (D, ) shaped numpy array containing the next weight vector\n",
    "                after updating on a single misclassified point, if one exists.\n",
    "        next_b: The next float bias term after updating on a single\n",
    "                misclassified point, if one exists.\n",
    "    \"\"\"\n",
    "    #==============================================\n",
    "    # TODO: Implement update rule for perceptron.\n",
    "    #===============================================\n",
    "\n",
    "    next_w, next_b = np.copy(w), np.copy(b)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        if Y[i] != predict(X[i], w, b):\n",
    "            next_w += Y[i] * X[i]\n",
    "            next_b += Y[i]\n",
    "            break\n",
    "        \n",
    "    return next_w, next_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_jIXhcxuPxf"
   },
   "source": [
    "Next you will fill in the `run_perceptron()` method. The method performs single updates on a misclassified point until convergence, or max_iter updates are made. The function will return the final weights and bias. You should use the `update_perceptron()` method you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AAlcLXgBuPxf"
   },
   "outputs": [],
   "source": [
    "def run_perceptron(X, Y, w, b, max_iter):\n",
    "    \"\"\"\n",
    "    This method runs the perceptron learning algorithm. Takes in initial weights\n",
    "    and runs max_iter update iterations. Returns final weights and bias.\n",
    "\n",
    "    Inputs:\n",
    "        X: A (N, D) shaped numpy array containing a single point.\n",
    "        Y: A (D, ) shaped numpy array containing the labels for the points.\n",
    "        w: A (D, ) shaped numpy array containing the initial weight vector.\n",
    "        b: A float containing the initial bias term.\n",
    "        max_iter: An int for the maximum number of updates evaluated.\n",
    "\n",
    "    Output:\n",
    "        w: A (D, ) shaped numpy array containing the final weight vector.\n",
    "        b: The final float bias term.\n",
    "    \"\"\"\n",
    "\n",
    "    #============================================\n",
    "    # TODO: Implement perceptron update loop.\n",
    "    #=============================================\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vrb5iTWAuPxf"
   },
   "source": [
    "# Problem A-ii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fsv6CYu2uPxf"
   },
   "source": [
    "## Visualizing a Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SifF-R03uPxf"
   },
   "source": [
    "We will begin by training our perceptron on a toy dataset of 3 points. The green points are labelled +1 and the red points are labelled -1. We use the helper function `plot_data()` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "oEwkMRvXuPxf"
   },
   "outputs": [],
   "source": [
    "X = np.array([[ -3, -1], [0, 3], [1, -2]])\n",
    "Y = np.array([ -1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SLNC5KLTuPxf"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = fig.gca(); ax.set_xlim(-4.1, 3.1); ax.set_ylim(-3.1, 4.1)\n",
    "plot_data(X, Y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEi-RVuiuPxf"
   },
   "source": [
    "## Running the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmZsqxBOuPxf"
   },
   "source": [
    "Next, we will run the perceptron learning algorithm on this dataset. Update the code to show the weights and bias at each timestep and the misclassified point used in each update. You may change the `update_perceptron()` method to do this, but be sure to update the starter code as well to reflect those changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyJ63qgZuPxf"
   },
   "source": [
    "Run the below code, and fill in the corresponding table in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yKLG12XxuPxf"
   },
   "outputs": [],
   "source": [
    "# Initialize weights and bias.\n",
    "weights = np.array([0.0, 1.0])\n",
    "bias = 0.0\n",
    "\n",
    "weights, bias = run_perceptron(X, Y, weights, bias, 16)\n",
    "\n",
    "print()\n",
    "print (\"final w = %s, final b = %.1f\" % (weights, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKcBkMRCuPxf"
   },
   "source": [
    "## Visualizating the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hDEg0tUuPxf"
   },
   "source": [
    "Getting all that information in table form isn't very informative. Let us visualize what the decision boundaries are at each timestep instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWD8sOj3uPxf"
   },
   "source": [
    "The helper functions `boundary()` and `plot_perceptron()` plot a decision boundary given a perceptron weights and bias. Note that the equation for the decision boundary is given by:\n",
    "\n",
    "$$w_1x_1 + w_2x_2 + b = 0.$$\n",
    "\n",
    "Using some algebra, we can obtain $x_2$ from $x_1$ to plot the boundary as a line.\n",
    "\n",
    "$$x_2 = \\frac{-w_1x_2 - b}{w_2}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8OrFnpPuPxg"
   },
   "source": [
    "Below is a redefinition of the `run_perceptron()` method to visualize the points and decision boundaries at each timestep instead of printing.  Fill in the method using your previous `run_perceptron()` method, and the above helper methods.\n",
    "\n",
    "Hint: The axs element is a list of Axes, which are used as subplots for each timestep. You can  do the following:\n",
    "```\n",
    "ax = axs[i]\n",
    "```\n",
    "to get the plot correponding to $t = i$. You can then use ax.set_title() to title each subplot. You will want to use the `plot_data()` and `plot_perceptron()` helper methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7FK-6JRUuPxg"
   },
   "outputs": [],
   "source": [
    "def run_perceptron(X, Y, w, b, axs, max_iter):\n",
    "    \"\"\"\n",
    "    This method runs the perceptron learning algorithm. Takes in initial weights\n",
    "    and runs max_iter update iterations. Returns final weights and bias.\n",
    "\n",
    "    Inputs:\n",
    "        X: A (N, D) shaped numpy array containing a single point.\n",
    "        Y: A (N, ) shaped numpy array containing the labels for the points.\n",
    "        w: A (D, ) shaped numpy array containing the initial weight vector.\n",
    "        b: A float containing the initial bias term.\n",
    "        axs: A list of Axes that contain suplots for each timestep.\n",
    "        max_iter: An int for the maximum number of updates evaluated.\n",
    "\n",
    "    Output:\n",
    "        The final weight and bias vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    #============================================\n",
    "    # TODO: Implement perceptron update loop.\n",
    "    #=============================================\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DJ1z8BVuPxg"
   },
   "source": [
    "Run the below code to get a visualization of the perceptron algorithm. The red region are areas the perceptron thinks are negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Zaa_Mwk3uPxg"
   },
   "outputs": [],
   "source": [
    "# Initialize weights and bias.\n",
    "weights = np.array([0.0, 1.0])\n",
    "bias = 0.0\n",
    "\n",
    "f, ax_arr = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(9,8))\n",
    "axs = list(itertools.chain.from_iterable(ax_arr))\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-4.1, 3.1); ax.set_ylim(-3.1, 4.1)\n",
    "\n",
    "run_perceptron(X, Y, weights, bias, axs, 4)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPJ0WAQTuPxg"
   },
   "source": [
    "# Problem A-iii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BROffW_DuPxg"
   },
   "source": [
    "## Visualize a Non-linearly Separable Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fySZoZEPuPxg"
   },
   "source": [
    "We will now work on a dataset that cannot be linearly separated, namely one that is generated by the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "sedzZHEQuPxg"
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "Y = np.array([1, 1, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "mjdfc6BouPxg"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "ax = fig.gca(); ax.set_xlim(-0.1, 1.1); ax.set_ylim(-0.1, 1.1)\n",
    "plot_data(X, Y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "166GulLUuPxg"
   },
   "source": [
    "We will now run the perceptron algorithm on this dataset. We will limit the total timesteps this time, but you should see a pattern in the updates. Run the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2UN55jSUuPxg"
   },
   "outputs": [],
   "source": [
    "# Initialize weights and bias.\n",
    "weights = np.array([0.0, 1.0])\n",
    "bias = 0.0\n",
    "\n",
    "f, ax_arr = plt.subplots(4, 4, sharex=True, sharey=True, figsize=(9,8))\n",
    "axs = list(itertools.chain.from_iterable(ax_arr))\n",
    "for ax in axs:\n",
    "    ax.set_xlim(-0.1, 1.1); ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "run_perceptron(X, Y, weights, bias, axs, 16)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gxjzWP9uPxg"
   },
   "source": [
    "# Problem B\n",
    "\n",
    "Implement MNIST Classification with MLP. Your `train_mnist` function should return the trained model.\n",
    "\n",
    "Hint: use the `torchvision` library to load the [MNIST dataset](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NBKYu-1uhk_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #======================================\n",
    "        # TODO: Define model instance variables\n",
    "        #======================================\n",
    "\n",
    "    def forward(self, x):\n",
    "        #===========================\n",
    "        # TODO: Forward pass for MLP\n",
    "        #===========================\n",
    "        return x\n",
    "\n",
    "def train_mnist(device = 'cpu'):\n",
    "    \"\"\"\n",
    "    This method performs end to end training of a model for the MNIST\n",
    "    classification task. Returns the final trained model, which should have\n",
    "    one intermediate layer with 500 hidden units. Feel free to write helper\n",
    "    functions/classes.\n",
    "    \"\"\"\n",
    "    model = MLP().to(device)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    #==============================================================\n",
    "    # TODO: Implement the MLP model, dataloading, and training loop\n",
    "    # for MNIST classification.\n",
    "    #==============================================================\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
