{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Data has 613 characters, 37 unique\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = open(\"data2.txt\", 'r').read()\n",
    "chars = sorted(list(set(data)))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Data has {} characters, {} unique\".format(data_size, vocab_size))\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "# char to index and index to char maps\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data from chars to indices\n",
    "data = list(data)\n",
    "for i, ch in enumerate(data):\n",
    "    data[i] = char_to_ix[ch]\n",
    "\n",
    "data = torch.tensor(data).to(device)\n",
    "# data = torch.unsqueeze(data, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, output_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        embedding = self.embedding(input_seq)\n",
    "        output, hidden_state = self.rnn(embedding, hidden_state)\n",
    "        output = self.decoder(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, (hidden_state[0].detach(), hidden_state[1].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=vocab_size, embedding_size=vocab_size, output_size=vocab_size, hidden_size=100).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/1wy0mfcn06dgzx7y03scmf5w0000gn/T/ipykernel_34813/1868735736.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Loss: 3.60957168\n",
      "Epoch: 2 \t Loss: 3.60707596\n",
      "Epoch: 3 \t Loss: 3.57710017\n",
      "Epoch: 4 \t Loss: 3.52508116\n",
      "Epoch: 5 \t Loss: 3.51086148\n",
      "Epoch: 6 \t Loss: 3.50273147\n",
      "Epoch: 7 \t Loss: 3.49558198\n",
      "Epoch: 8 \t Loss: 3.48925955\n",
      "Epoch: 9 \t Loss: 3.48403896\n",
      "Epoch: 10 \t Loss: 3.47984969\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i_epoch in range(1, epochs+1):\n",
    "        \n",
    "    n = 0\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i in range(40,len(data)-1):\n",
    "        hidden_state = None\n",
    "        input_seq = data[i-40 : i]\n",
    "        target_seq = data[i-40+1 : i+1]\n",
    "        \n",
    "        # forward pass\n",
    "        output, _ = model(input_seq, hidden_state)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "        \n",
    "        # compute gradients and take optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # print loss after every epoch\n",
    "    print(\"Epoch: {0} \\t Loss: {1:.8f}\".format(i_epoch, running_loss/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " t t t t t t t t t t t t t t t t t t t t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/1wy0mfcn06dgzx7y03scmf5w0000gn/T/ipykernel_34813/1868735736.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"shall i compare thee to a summersr dayy\\n\"\n",
    "\n",
    "prompt = list(prompt)\n",
    "for i, ch in enumerate(prompt):\n",
    "    prompt[i] = char_to_ix[ch]\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt = torch.tensor(prompt).to(device).long()\n",
    "    hidden_init = None\n",
    "    output, hidden = model(prompt, hidden_init)\n",
    "\n",
    "    for _ in range(40):\n",
    "        output = output[-1]\n",
    "        prediction = torch.argmax(output)\n",
    "        print(ix_to_char[int(prediction.detach().numpy())],end=\"\")\n",
    "        output, hidden = model(torch.tensor([prediction]), hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
